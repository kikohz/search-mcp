#!/usr/bin/env python3
"""
æœç´¢ MCP æœåŠ¡å™¨ - ä½¿ç”¨ SearXNG ä½œä¸ºåç«¯
æä¾›å…è´¹ã€éšç§ä¿æŠ¤çš„ç½‘ç»œæœç´¢åŠŸèƒ½
"""

import sys
import json
import urllib.request
import urllib.parse

# SearXNG æœåŠ¡å™¨é…ç½®
SEARXNG_URL = "http://100.126.219.109:7070"

# MCP å·¥å…·å®šä¹‰
TOOLS = [
    {
        "name": "web_search",
        "description": "æœç´¢ç½‘ç»œä¿¡æ¯",
        "inputSchema": {
            "type": "object",
            "properties": {
                "query": {"type": "string", "description": "æœç´¢å…³é”®è¯"},
                "max_results": {"type": "integer", "description": "è¿”å›ç»“æœæ•°é‡ (é»˜è®¤ 10ï¼Œæœ€å¤š 30)", "default": 10}
            },
            "required": ["query"]
        }
    },
    {
        "name": "news_search",
        "description": "æœç´¢æ–°é—»",
        "inputSchema": {
            "type": "object",
            "properties": {
                "query": {"type": "string", "description": "æœç´¢å…³é”®è¯"},
                "max_results": {"type": "integer", "description": "è¿”å›ç»“æœæ•°é‡ (é»˜è®¤ 10)", "default": 10}
            },
            "required": ["query"]
        }
    },
    {
        "name": "search_summary",
        "description": "æœç´¢å¹¶ç”Ÿæˆæ‘˜è¦",
        "inputSchema": {
            "type": "object",
            "properties": {
                "query": {"type": "string", "description": "æœç´¢å…³é”®è¯"}
            },
            "required": ["query"]
        }
    }
]

def searxng_search(query, categories=None, max_results=10):
    """è°ƒç”¨ SearXNG API æœç´¢"""
    try:
        # æ„å»º URL
        params = {
            'q': query,
            'format': 'json',
            'pageno': 1,
            'language': 'zh-CN'
        }
        
        if categories:
            params['categories'] = categories
        
        url = f"{SEARXNG_URL}/search?{urllib.parse.urlencode(params)}"
        
        # å‘é€è¯·æ±‚
        req = urllib.request.Request(url)
        req.add_header('User-Agent', 'Mozilla/5.0 (compatible; SearchMCP/1.0)')
        
        with urllib.request.urlopen(req, timeout=10) as response:
            data = json.loads(response.read().decode('utf-8'))
        
        # æå–ç»“æœ
        results = data.get('results', [])[:max_results]
        
        return results
    
    except Exception as e:
        return [{"error": f"æœç´¢å¤±è´¥ï¼š{str(e)}"}]

def web_search(query, max_results=10):
    """æœç´¢ç½‘é¡µ"""
    max_results = min(max_results, 30)
    results = searxng_search(query, categories='general', max_results=max_results)
    
    formatted = []
    for r in results:
        if 'error' in r:
            return [r]
        formatted.append({
            "title": r.get("title", ""),
            "url": r.get("url", ""),
            "snippet": r.get("content", ""),
            "source": r.get("engine", ""),
            "publishedDate": r.get("publishedDate", "")
        })
    
    return formatted

def news_search(query, max_results=10):
    """æœç´¢æ–°é—»"""
    max_results = min(max_results, 30)
    results = searxng_search(query, categories='news', max_results=max_results)
    
    formatted = []
    for r in results:
        if 'error' in r:
            return [r]
        formatted.append({
            "title": r.get("title", ""),
            "url": r.get("url", ""),
            "snippet": r.get("content", ""),
            "source": r.get("engine", ""),
            "publishedDate": r.get("publishedDate", "")
        })
    
    return formatted

def search_summary(query):
    """æœç´¢å¹¶ç”Ÿæˆæ‘˜è¦"""
    results = searxng_search(query, categories='general', max_results=10)
    
    if not results or 'error' in results[0]:
        return "æœªæ‰¾åˆ°ç›¸å…³ç»“æœ"
    
    summary = f"æœç´¢ã€Œ{query}ã€æ‰¾åˆ° {len(results)} ä¸ªç»“æœï¼š\n\n"
    for i, r in enumerate(results, 1):
        summary += f"{i}. **{r.get('title', '')}**\n"
        summary += f"   {r.get('content', '')}\n"
        summary += f"   æ¥æºï¼š{r.get('engine', '')}\n"
        if r.get('publishedDate'):
            summary += f"   æ—¥æœŸï¼š{r.get('publishedDate')}\n"
        summary += f"   é“¾æ¥ï¼š{r.get('url', '')}\n\n"
    
    return summary

if __name__ == "__main__":
    print("ğŸ” æœç´¢ MCP æœåŠ¡å™¨ (SearXNG)")
    print("=" * 50)
    print(f"SearXNG æœåŠ¡å™¨ï¼š{SEARXNG_URL}")
    print("=" * 50)
    print("")
    print("å¯ç”¨å·¥å…·:")
    for tool in TOOLS:
        print(f"  - {tool['name']}: {tool['description']}")
    print("=" * 50)
    print("")
    print("è¿™æ˜¯ä¸€ä¸ª MCP æœåŠ¡å™¨ï¼Œéœ€è¦é€šè¿‡ MCP å®¢æˆ·ç«¯è°ƒç”¨ã€‚")
    print("")
    print("Claude Desktop é…ç½®ç¤ºä¾‹:")
    print(json.dumps({
        "mcpServers": {
            "search": {
                "command": "python3",
                "args": ["/root/.openclaw/workspace/mcp-search-server/search_mcp.py"]
            }
        }
    }, indent=2))
    print("")
    print("æµ‹è¯•è°ƒç”¨:")
    print("  web_search(query='Python æ•™ç¨‹', max_results=5)")
    print("")
    
    # ç®€å•æµ‹è¯•
    print("è¿è¡Œæµ‹è¯•...")
    result = web_search("Python æ•™ç¨‹", 3)
    print(f"æµ‹è¯•ç»“æœï¼š{len(result)} æ¡ç»“æœ")
    for r in result:
        print(f"  - {r.get('title', 'N/A')}")
