#!/usr/bin/env python3
"""æœç´¢ MCP æœåŠ¡å™¨ - ä½¿ç”¨ SearXNG åç«¯"""

import sys
import urllib.request
import urllib.parse
import json

# SearXNG æœåŠ¡å™¨åœ°å€
SEARXNG_URL = "http://100.126.219.109:7070"

def searxng_search(query, max_results=10):
    """è°ƒç”¨ SearXNG API"""
    try:
        params = {'q': query, 'format': 'json'}
        url = f"{SEARXNG_URL}/search?{urllib.parse.urlencode(params)}"
        req = urllib.request.Request(url)
        req.add_header('User-Agent', 'Mozilla/5.0')
        with urllib.request.urlopen(req, timeout=10) as response:
            data = json.loads(response.read().decode('utf-8'))
        return data.get('results', [])[:max_results]
    except Exception as e:
        return [{"error": f"æœç´¢å¤±è´¥ï¼š{str(e)}"}]

def web_search(query, max_results=5):
    """æœç´¢ç½‘é¡µ"""
    results = searxng_search(query, min(max_results, 30))
    return [{"title": r.get("title", ""), "url": r.get("url", ""), "snippet": r.get("content", ""), "source": r.get("engine", "")} for r in results]

def news_search(query, max_results=5):
    """æœç´¢æ–°é—»"""
    results = searxng_search(query, min(max_results, 30))
    return [{"title": r.get("title", ""), "url": r.get("url", ""), "snippet": r.get("content", ""), "source": r.get("engine", ""), "date": r.get("publishedDate", "")} for r in results]

def search_summary(query):
    """æœç´¢å¹¶ç”Ÿæˆæ‘˜è¦"""
    results = searxng_search(query, 10)
    if not results:
        return "æœªæ‰¾åˆ°ç›¸å…³ç»“æœ"
    summary = f"æœç´¢ã€Œ{query}ã€æ‰¾åˆ° {len(results)} ä¸ªç»“æœï¼š\n\n"
    for i, r in enumerate(results, 1):
        summary += f"{i}. **{r.get('title', '')}**\n   {r.get('content', '')}\n   æ¥æºï¼š{r.get('engine', '')}\n   é“¾æ¥ï¼š{r.get('url', '')}\n\n"
    return summary

if __name__ == "__main__":
    print("ğŸ” æœç´¢ MCP æœåŠ¡å™¨ (SearXNG)")
    print(f"æœåŠ¡å™¨ï¼š{SEARXNG_URL}")
    print("å·¥å…·ï¼šweb_search, news_search, search_summary")
